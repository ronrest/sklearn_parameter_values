from __future__ import print_function
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from time import time
import operator
# sys.path.append("/home/ronny/programming/machine_learning/courses/udacity_Intro_to_ML")  # for importing data and plotting
#sys.path.append("/home/ronny/programming/machine_learning/courses/udacity_Intro_to_ML/04_adaboost")  # for importing data and plotting
import numpy as np
#from supporting_functions.plots import plot_decision_boundary, visualise2D
#import matplotlib.pyplot as plt


#from class_vis import prettyPicture
from bokeh.plotting import figure, output_file, show, vplot



# ==============================================================================
#                                                                  PLOT SETTINGS
# ==============================================================================
BLUE = "#1A94D6"
ORANGE = "#FF8000"
GREEN = "#73AD21"

BLUE_PALE = "#A2C4DA"
ORANGE_PALE = "#F8C381"
GREEN_PALE = "#BBF864"

COLORS = [BLUE,  ORANGE, GREEN]
COLORS_PALE = [BLUE_PALE,  ORANGE_PALE, GREEN_PALE]


#===============================================================================
#                                                      ADABOOST_WITH_SIMPLE_TREE
#===============================================================================
def adaboost_with_simple_tree(data_generator,
                              sample_size=1000,
                              max_depth=1,
                              n_estimators=50,
                              learning_rate=1.0,
                              random_state=None
                              ):
    """
    Trains an adaboost classifier using a Decision Tree as its weak classifier.
    Trains on the data generated by the

    :param data_generator: (int)
        A function object that generates toy data to evaluate the classifier.
        The function must be able to accept the sample size as its one and only
        argument, and must return its data as a tuple or list of the following
        4 elements in this exact order:
            X_train, Y_train, X_test, Y_test
    :param sample_size: (int)
        Size of the data to generate.
    :param max_depth: (int)
        How deep to make the weak classifier Decision Tree
    :param n_estimators: (int)
        Number of classifiers to use for adaboost
    :param learning_rate: (float)
        learning rate for adaboosts classifiers
    :param random_state: (int or None)
        set the seed for random number generator.
    :return: (dict)
        Returns a dictionary with the following keys:

        model          = The trained model,
        in_sample_acc  = The accuracy calculated using the training set.
        out_sample_acc = The accuracy calculated using the validation set.
        train_time     = The time it took diring the training phase.
    """
    #===========================================================================
    X_train, Y_train, X_test, Y_test = data_generator(sample_size)
    classifier = DecisionTreeClassifier(max_depth=max_depth)
    model = AdaBoostClassifier(base_estimator=classifier,
                               n_estimators=n_estimators,
                               learning_rate=learning_rate,
                               algorithm='SAMME.R',
                               random_state=random_state)


    # --------------------------------------------------------------------------
    #                                                   train boosted classifier
    # --------------------------------------------------------------------------
    t0 = time()
    model = model.fit(X_train, Y_train)
    train_time = time()- t0

    # --------------------------------------------------------------------------
    #                                                           Make predictions
    # --------------------------------------------------------------------------
    # t0 = time()
    # preds = model.predict(X_test)
    # pred_time = time()- t0

    # --------------------------------------------------------------------------
    #                                                                   Evaluate
    # --------------------------------------------------------------------------
    in_sample_accuracy = model.score(X_train, Y_train)
    out_of_sample_accuracy = model.score(X_test, Y_test)

    return {"model":model,
            "X_train":X_train,
            "Y_train":Y_train,
            "X_test":X_test,
            "Y_test":Y_test,
            "in_sample_acc":in_sample_accuracy,
            "out_sample_acc":out_of_sample_accuracy,
            "train_time":train_time}


# ==============================================================================
#                                                 LOOP_ADABOOST_WITH_SIMPLE_TREE
# ==============================================================================
def loop_adaboost_with_simple_tree(data_generator,
                                   sample_size=1000,
                                   max_depth=1,
                                   n_estimators=50,
                                   learning_rate=1.0,
                                   random_state=None
                                   ):
    # ==========================================================================
    largs = {"sample_size": sample_size,    # Loopable arguments
            "max_depth":max_depth,
            "n_estimators":n_estimators,
            "learning_rate":learning_rate}

    looper_id = None
    #print("looking for looper")
    for key in largs.keys():            # Find out which argument will be looped
        if isinstance(largs[key], list):
            looper_id = key
            looper_values = largs[looper_id]
            #print("found looper!!!")
            break
    if looper_id is None:
        msg = "\n    One of the arguments MUST be a list of values"
        raise ValueError(msg)
    #print("Looper is ", looper_id)


    num_iterations = len(largs[looper_id])
    running_results = {"in_sample_acc":[None]*num_iterations,
                       "out_sample_acc": [None] * num_iterations,
                       "train_time": [None] * num_iterations,
                      }

    for i, val in enumerate(looper_values):#largs[looper_id]):
        largs[looper_id] = val

        temp_results = adaboost_with_simple_tree(data_generator,
                                                 sample_size=largs["sample_size"],
                                                 max_depth=largs["max_depth"],
                                                 n_estimators=largs["n_estimators"],
                                                 learning_rate=largs["learning_rate"],
                                                 random_state=random_state
                                                 )

        # Append the values for each metric being measured  for this iteration
        # of running resuts
        for key in running_results.keys():
            running_results[key][i] = temp_results[key]

    best_setting, best_setting_accuracy = max(
            zip(looper_values, running_results["out_sample_acc"]),
            key=operator.itemgetter(1))

    running_results["best_setting"] = best_setting
    running_results["best_setting_accuracy"] = best_setting_accuracy

    print("\n----------------------------------------",
          "\nBest Results",
          "\n----------------------------------------"
          "\n" + looper_id + ":", best_setting,
          "\nOut of sample accuracy:", best_setting_accuracy,
          "\n----------------------------------------")


    return running_results




#
# # ==============================================================================
# #                                                                    SAMPLE SIZE
# # ==============================================================================
# sample_sizes = range(1000, 5000+1, 1000)
# sample_size_results = loop_adaboost_with_simple_tree(data_generator,
#                                sample_size=sample_sizes,
#                                max_depth=default_max_depth,
#                                n_estimators=default_n_estimators,
#                                learning_rate=default_learning_rate,
#                                random_state=seed
#                                )
#
#
# # ==============================================================================
# #                                                      PLOT SAMPLE SIZE SETTINGS
# # ==============================================================================
# output_file("/tmp/plotty.html", title="myPlot")
# s1 = figure(title="Sample Size vs Accuracy", x_axis_label='Sample Size', y_axis_label='Accuracy')
# s1.line(sample_sizes, sample_size_results["in_sample_acc"], legend="In sample Accuracy", line_width=2, color=BLUE)
# s1.line(sample_sizes, sample_size_results["out_sample_acc"], legend="Out of sample Accuracy", line_width=2, color=ORANGE)
# s1.logo = None
# s1.toolbar_location = None
#
# s2 = figure(title="Sample Size vs Training Time", x_axis_label='Sample Size', y_axis_label='Training Time (secs)')
# s2.line(sample_sizes, sample_size_results["train_time"], line_width=2, color=ORANGE)
# s2.logo = None
# s2.toolbar_location = None
# p = hplot(s1, s2)
# show(p)






#===============================================================================
#                                                                  Print Details
#===============================================================================
# print("--------------------------------")
# print("max_depth= ", max_depth)
# print("n_estimators= ", n_estimators)
# print("learning_rate= ", learning_rate)
# print("--------------------------------")
# print("Number of training samples: ", X_train.shape[0])
# print("Number of features: ", X_train.shape[1])
# print("--------------------------------")
# print("Accuracy of Adaboost: ", accuracy)
# print("Accuracy of weak classifier: ", classifier_independent_accuracy)
# print("Training time:", round(train_time, 3), "s")
# print("Predictions time:", round(pred_time, 3), "s")
# print("--------------------------------")

#
# #===============================================================================
# #                                                    PLOT DECISION BOUNDARY (2D)
# #===============================================================================
# #prettyPicture(clf, X_test, y_test)
#
# #------------------------------------------------------------  SETUP PLOT STYLES
# # Style for the Figure title
# sup_title_style = {"fontweight": "extra bold",
#                    "style": "normal",
#                    "color": "#333333",
#                    "size": 20,
#                    "family": "serif"}
#
# # Style for the sub-plot title
# title_style = {"fontweight": "book",  # int, "book", "black", "ultralight"
#                "style": "italic",
#                "color": "#333333",
#                "size": 14,
#                "family": "serif"}
#
# #------------------------------------------------------------------- START PLOTS
# num_rows = 1
# num_cols = 2
#
# # Main title
# plt.suptitle('Adaboost vs Weak Classifier', fontdict=sup_title_style)
#
# plt.subplot(num_rows, num_cols, 1)    # 1 Row, 2 Columns, activate the 2nd cell
# plot_decision_boundary(X_train, Y_train, model.predict, resolution=[0.01, 0.01])
# plt.title("Adaboost Classifier", loc="left", fontdict=title_style)
#
# plt.subplot(num_rows, num_cols, 2)    # 1 Row, 2 Columns, activate the 2nd cell
# plot_decision_boundary(X_train, Y_train, classifier_independent.predict, resolution=[0.01, 0.01])
# plt.title("Weak Classifier", loc="left", fontdict=title_style)
#

def parameter_plots(x, results_dict, x_label, title_accuracy, title_time,
                    legend_pos="top_right"):
    """
    :param x: (list)
        The parameter values (used as the x axis)
    :param results_dict: (dict)
        The results dictionary returned by the looping parameters function
    :param x_label: (string)
        Label to use on the x axis
    :param title_accuracy: (string)
        Title For the Accuracy Plot
    :param title_time: (string)
        Title for the training time plot
    :param legend_pos: (string)
        Position of the legend
    """
    # =================
    # MAX DEPTH PLOTS
    # =================
    # --------------
    # ACCURACY PLOT
    # --------------
    s1 = figure(title=title_accuracy, width=600, height=400,
                x_axis_label=x_label,
                y_axis_label='Accuracy')
    s1.line(x, results_dict["in_sample_acc"],
            legend="In sample Accuracy", line_width=2, color=BLUE)
    s1.line(x, results_dict["out_sample_acc"],
            legend="Out of sample Accuracy", line_width=2, color=ORANGE)
    s1.legend.location = legend_pos
    s1.logo = None
    s1.toolbar_location = None

    # ---------------
    # TRAIN TIME PLOT
    # ---------------
    s2 = figure(title=title_time, width=600, height=400,
                x_axis_label=x_label,
                y_axis_label='Training Time (secs)')
    s2.line(x, results_dict["train_time"],
            line_width=2, color=ORANGE)
    s2.logo = None
    s2.toolbar_location = None

    # ----------------
    # VERTICALLY STACK
    # ----------------
    p = vplot(s1, s2)
    show(p)


